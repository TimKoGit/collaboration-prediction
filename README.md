# Collaboration Prediction

## Смысловое содержание проекта

### Постановка задачи

Имея граф коллаборации авторов статей, необходимо научиться предсказывать, какие ученые будут работать вместе в будущем. Проект представляет собой специализированную систему рекомендаций для научного сообщества.

### Формат данных

- **Входные данные**: Статический граф, где вершины — это авторы статей, а ребра — коллаборации между ними.
  - Каждому автору соответствует 128-мерный эмбеддинг, отражающий тематику его работ.
  - На вход также подается список пар вершин (ребер), для которых нужно предсказать вероятность будущей коллаборации.
- **Выходные данные**: Вероятность возникновения связи для каждой из переданных пар.

### Метрики

Задача решается как бинарная классификация в условиях сильного дисбаланса классов.

- **Основная метрика**: `Hits@50` (классическая метрика для задач Link Prediction).
  - `Hits@k` рассчитывается следующим образом: все предсказанные «негативные» ребра (где связи нет) сортируются по уверенности модели. Берется порог на уровне k-го по уверенности ребра, и подсчитывается, сколько «положительных» ребер получили оценку выше этого порога.
- **Дополнительная метрика**: `AUROC`.

### Валидация и тест

Поскольку граф статичен, во всех выборках (обучающей, валидационной и тестовой) используются одни и те же вершины с фиксированными эмбеддингами. Ребра распределяются по выборкам, при этом количество негативных ребер ограничивается для предотвращения критического дисбаланса при обучении.

### Датасеты

Проект использует датасет **OGB (Open Graph Benchmark) - ogbl-collab** ([подробнее](https://ogb.stanford.edu/docs/linkprop/#ogbl-collab)):

- **Источник**: Граф ученых из Microsoft Academic Graph.
- **Характеристики**: 235,868 вершин (авторов), 128-мерные эмбеддинги, 2,358,104 ребер (коллабораций).
- **Размер**: ~350 Мб.
- **Задача**: Предсказание будущих связей на основе временных срезов.

### Моделирование

#### Архитектура

Используется Message-Passing архитектура (GraphSAGE), работающая поверх:

- **Контентных эмбеддингов** вершин (128-мерные векторы из датасета).
- **Структурных эмбеддингов DeepWalk**: 128-мерные векторы, полученные через случайные блуждания и Skip-gram (реализовано на чистом PyTorch).
- **Anchor Encodings**: Дистанционные признаки, вычисленные относительно якорных вершин графа.

Все признаки объединяются в единый входной вектор для энкодера узлов. Поскольку граф статичен, вычисление структурных признаков выполняется один раз на этапе предобработки.

**Оптимизатор**: Adam.

### Внедрение

Модель внедрена в виде сервиса с REST API. Доступна возможность проверки вероятности коллаборации между двумя конкретными учеными.

---

## Технические детали работы с проектом

### Setup

#### Требования

- **Python**: >= 3.13
- **uv**: менеджер зависимостей и окружений
- **Docker**: для запуска Triton Inference Server (опционально)
- **CUDA**: для GPU-ускорения (опционально)

#### Установка окружения

1. **Клонируйте репозиторий:**

   ```bash
   git clone https://github.com/TimKoGit/collaboration-prediction.git
   cd collaboration-prediction
   ```

2. **Установите пакет и зависимости:**

Для установки всех зависимостей для полноценной работы с репозиторием:

```bash
uv sync --extra export --extra inference --group dev
```

Для установки только основных зависимотей для обучения и инференса модели:

```bash
uv sync
```

Это установит:

- Пакет `collaboration-prediction` с CLI командой `collab-pred`
- PyTorch и PyTorch Lightning
- Hydra для конфигурации
- DVC для версионирования данных
- MLflow для логирования экспериментов
- OGB для работы с датасетами
- И другие базовые зависимости

После установки команда `collab-pred` будет доступна в вашем окружении.

3. **Установите зависимости для экспорта моделей (опционально):**

   ```bash
   uv sync --extra export
   ```

   Это установит:
   - `onnx` и `onnxruntime` для экспорта в ONNX
   - `onnxscript` для работы с ONNX

4. **Установите зависимости для inference сервера (опционально):**

   ```bash
   uv sync --extra inference
   ```

   Это установит:
   - `fastapi` и `uvicorn` для FastAPI сервера
   - `tritonclient` для интеграции с Triton Inference Server
   - `pydantic` для валидации данных

5. **Установите зависимости для разработки (опционально):**

   ```bash
   uv sync --group dev
   ```

   Это установит инструменты для форматирования и линтинга кода.

6. **Установите pre-commit хуки (рекомендуется для разработки):**

   ```bash
   uv run pre-commit install
   ```

   Это настроит автоматическую проверку и форматирование кода (black, isort, flake8) при каждом `git commit`.

#### Проверка установки

После установки вы должны иметь возможность:

- Запускать команды через CLI: `collab-pred --help`
- Импортировать пакет: `python -c "import collaboration_prediction"`

---

### Train

#### Подготовка данных

Проект использует внешнюю папку для хранения данных и DVC репозитория. Это позволяет не загромождать основной репозиторий тяжелыми файлами.

**Перед началом работы необходимо выполнить инициализацию данных:**

1.  **Настройте путь к DVC репозиторию:**
    В файле `configs/data/default.yaml` установите параметр `dvc_repo_path` (по умолчанию `./external_dvc_repo`).

2.  **Запустите команду инициализации:**
    ```bash
    collab-pred init-data
    ```
    Эта команда:
    - Инициализирует DVC репозиторий в указанной папке (в режиме no-SCM).
    - Скачивает датасет **ogbl-collab**.
    - Добавляет данные под контроль DVC.

**Примечание:** Команду `init-data` не обязательно запускать, если данные уже скачаны и находятся в папке, указанной в `data_root`. Однако, если данные отсутствуют (даже если DVC репозиторий уже инициализирован), запуск этой команды скачает их и настроит отслеживание.

#### Отслеживание экспериментов (MLflow)

Проект использует MLflow для логирования метрик и параметров. Перед запуском тренировки рекомендуется запустить MLflow сервер:

```bash
mlflow server --host 127.0.0.1 --port 8080
```

После запуска интерфейс будет доступен по адресу [http://127.0.0.1:8080](http://127.0.0.1:8080).

#### Запуск тренировки

**Перед запуском убедитесь, что вы выполнили `collab-pred init-data`!**

**Базовый запуск:**

```bash
collab-pred train
```

**С переопределением параметров через командную строку:**

```bash
collab-pred train training.batch_size=128 training.learning_rate=0.0005
collab-pred train model.hidden_dim=512 training.num_epochs=100
```

**С указанием другого конфига:**

```bash
collab-pred train --config-name my_config training.batch_size=64
```

#### Конфигурация тренировки

Все параметры настраиваются через Hydra конфигурацию в `configs/`:

- **`configs/config.yaml`** — главный конфигурационный файл
- **`configs/model/default.yaml`** — параметры модели (размерность скрытых слоев, количество слоев, dropout)
- **`configs/training/default.yaml`** — параметры обучения (learning rate, batch size, количество эпох)
- **`configs/data/default.yaml`** — параметры данных (имя датасета, путь к данным) и **структурных признаков** (DeepWalk, Anchor Encodings)
- **`configs/trainer/default.yaml`** — параметры PyTorch Lightning Trainer
- **`configs/logging/default.yaml`** — настройки логирования и MLflow

Вы можете изменить значения по умолчанию в этих файлах или переопределять их через командную строку.

#### Результаты тренировки

После тренировки вы найдете:

- **Checkpoints модели**: `lightning_logs/checkpoints/`
  - `best.ckpt` — лучшая модель по валидационной метрике
  - `last.ckpt` — последний checkpoint
- **Логи MLflow**: `lightning_logs/mlruns/` — метрики, параметры, артефакты
- **Графики обучения**: `plots/` — визуализация loss, метрик валидации

#### Метрики

Модель оценивается по следующим метрикам:

- **Hits@K** — доля положительных ребер, попавших в топ-K предсказаний
- **AUROC** — Area Under ROC Curve

  Эти метрики логируются в MLflow и отображаются в консоли во время обучения.

---

### Evaluate

Команда `evaluate` позволяет проверить качество обученной модели на тестовом наборе данных, используя сохраненный checkpoint.

**Запуск оценки:**

```bash
collab-pred evaluate --checkpoint lightning_logs/checkpoints/best.ckpt
```

**С переопределением параметров конфигурации:**

```bash
collab-pred evaluate \
  --checkpoint lightning_logs/checkpoints/best.ckpt \
  training.batch_size=256
```

**Что делает команда:**

1. Загружает статический граф и тестовые выборки.
2. Восстанавливает состояние модели из указанного `.ckpt` файла.
3. Выполняет проход по тестовому датасету и вычисляет итоговые метрики (`Hits@K`, `AUROC`).
4. Выводит результаты в консоль в удобном формате.

---

### Production Preparation

После успешной тренировки модель необходимо подготовить к продакшену. Процесс включает экспорт в оптимизированные форматы и настройку inference сервера.

#### Шаг 1: Экспорт в ONNX

**Экспорт обученной модели:**

```bash
collab-pred export-onnx \
  --checkpoint lightning_logs/checkpoints/best.ckpt \
  --output models/link_prediction.onnx
```

Команда:

1. Загружает модель из checkpoint
2. Загружает пример данных из датасета для определения входных/выходных размеров
3. Экспортирует модель в ONNX с правильными спецификациями входов/выходов
4. Сохраняет ONNX модель в указанный путь

**Важно:** Модель экспортируется со статическими размерами для `edges` ([1, 2]) и `predictions` ([1, 1]), что позволяет обрабатывать по одному ребру за запрос.

#### Проверка ONNX модели

После экспорта можно проверить, что ONNX модель работает так же, как оригинальная PyTorch модель (по умолчанию после экспорта модели проверка запускается автоматически).

Мотивация этой проверки состоит в том, что при переносе в ONNX формат пришлось убрать работу с разреженными матрицами, из-за чего немного помнять архитектуру.

**Проверка модели:**

```bash
collab-pred verify-onnx \
  --onnx-model models/link_prediction.onnx \
  --checkpoint lightning_logs/checkpoints/best.ckpt
```

Команда:

1. Загружает оригинальную PyTorch модель из checkpoint
2. Загружает пример данных из датасета
3. Запускает inference на обеих моделях (PyTorch и ONNX)
4. Сравнивает выходы и проверяет, что они совпадают в пределах допустимой погрешности

**Результат проверки:**

- ✅ **Успех**: Если выходы совпадают (по умолчанию: относительная погрешность `rtol=1e-3`, абсолютная `atol=1e-5`), вы увидите сообщение `✓ ONNX model verification passed!` с информацией о максимальной и средней разнице между выходами.
- ❌ **Ошибка**: Если выходы не совпадают, вы увидите детальную информацию о различиях, включая топ-5 наибольших расхождений.

**Автоматическая проверка после экспорта:**

По умолчанию проверка выполняется автоматически после экспорта (если в конфигурации `export.verify_after_export=true`). Если проверка не прошла, экспорт все равно завершится, но будет показано предупреждение.

#### Шаг 2: Подготовка модели для Triton Inference Server

**Подготовка модели:**

```bash
collab-pred prepare-triton \
  --onnx-model models/link_prediction.onnx \
  --model-repository ./models/triton_models \
  --model-name link_prediction
```

#### Комплектация поставки модели

Для запуска inference в продакшене необходимы следующие артефакты:

**Обязательные:**

1. **ONNX модель**: `models/link_prediction.onnx`
2. **Triton конфигурация**: `models/triton_models/link_prediction/config.pbtxt`
3. **Граф данных** (статический):
   - `node_features`: признаки узлов графа (загружается один раз при старте сервера)
   - `edge_index`: структура графа (загружается один раз)
   - `node_degrees`: степени узлов (загружается один раз)
4. **Docker образ с Triton Inference Server**
5. **FastAPI сервер для HTTP API**

---

### Infer

После подготовки модели можно запустить inference на новых данных.

**Запуск Triton Inference Server**

Triton должен быть запущен в Docker контейнере:

```bash
docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 \
  -v $(pwd)/models/triton_models:/models \
  nvcr.io/nvidia/tritonserver:24.01-py3 \
  tritonserver --model-repository=/models
```

**Проверка запуска:**

- HTTP API: `http://localhost:8000`
- gRPC API: `localhost:8001`
- Metrics: `http://localhost:8002/metrics`

**Шаг 2: Запуск FastAPI сервера**

В новом терминале:

```bash
collab-pred serve \
  --triton-url localhost:8000 \
  --triton-model-name link_prediction \
  --host 0.0.0.0 \
  --port 8080
```

Сервер:

- Загружает статический граф один раз при старте (node_features, edge_index, node_degrees)
- Предоставляет HTTP API для предсказаний
- Обрабатывает одно ребро за запрос

**Шаг 3: Использование API**

**API Endpoints:**

- **Документация API**: `http://localhost:8080/docs` (Swagger UI)
- **Health Check**: `GET http://localhost:8080/health`
- **Информация о модели**: `GET http://localhost:8080/model/info`
- **Предсказание**: `POST http://localhost:8080/predict`

**Формат данных для предсказания:**

Для статического графа требуется только одно ребро-кандидат. Граф загружается один раз при старте сервера.

**Пример запроса через curl:**

```bash
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"edge": [0, 1]}'
```

**Формат ответа:**

```json
{
  "prediction": 0.123
}
```

Где:

- `prediction` — вероятность возникновения связи (значение от 0 до 1)

**Важные замечания:**

- Ребро имеет формат `[src_node_id, dst_node_id]`, где node IDs — индексы в предзагруженном графе
- За один запрос обрабатывается ровно одно ребро.

---

## Структура проекта

```
collaboration_prediction/
├── __init__.py                    # Инициализация пакета
├── commands.py                    # CLI точка входа для всех команд
├── train.py                       # Скрипт тренировки
├── data_modules/
│   ├── __init__.py                # Инициализация модулей данных
│   ├── data.py                    # Загрузка и предобработка данных
│   ├── data_download.py           # Утилиты для скачивания данных
│   └── features.py                # Генерация структурных признаков (DeepWalk, Anchor)
├── model/
│   ├── __init__.py                # Инициализация моделей
│   ├── model.py                   # Компоненты нейронной сети
│   └── lightning_module.py        # PyTorch Lightning модуль
├── inference_modules/
│   ├── __init__.py                # Инициализация модулей инференса
│   ├── export.py                  # Утилиты экспорта (ONNX)
│   ├── inference.py               # Клиенты для inference (Triton)
│   └── inference_server.py        # FastAPI inference сервер
└── utils/
    └── dvc.py                     # Утилиты для работы с DVC

configs/
├── config.yaml                    # Главная Hydra конфигурация
├── model/                         # Конфигурация модели
│   └── default.yaml
├── training/                      # Конфигурация обучения
│   └── default.yaml
├── data/                          # Конфигурация данных
│   └── default.yaml
├── trainer/                       # Конфигурация PyTorch Lightning Trainer
│   └── default.yaml
├── export/                        # Конфигурация экспорта (ONNX)
│   └── default.yaml
└── logging/                       # Конфигурация логирования
    └── default.yaml

models/
└── triton_models/                 # Triton модель репозиторий
    └── link_prediction/
        ├── config.pbtxt           # Конфигурация Triton
        └── 1/
            └── model.onnx         # ONNX модель
```
